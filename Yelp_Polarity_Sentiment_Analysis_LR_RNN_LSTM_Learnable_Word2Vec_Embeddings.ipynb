{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMrX4NMa1YWXFz47WOHDGXf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tinayiluo0322/Yelp-Polarity-Sentiment-Analysis-Using-Deep-Learning-Models-With-Different-Embedding-Methods/blob/main/Yelp_Polarity_Sentiment_Analysis_LR_RNN_LSTM_Learnable_Word2Vec_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Yelp Polarity Sentiment Analysis Using Deep Learning Models With Different Embedding Methods\n",
        "\n",
        "#### Luopeiwen Yi"
      ],
      "metadata": {
        "id": "C_khutpV26qO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Introduction\n",
        "Sentiment classification is a fundamental task in natural language processing (NLP), with applications in customer feedback analysis, brand reputation monitoring, and automated content moderation. In this study, we implemented and compared multiple deep learning and traditional machine learning models for binary sentiment classification on the Yelp Polarity dataset. The objective was to evaluate how different architectures and embedding strategies impact classification performance. The models tested include Logistic Regression, Vanilla RNN, and LSTM, each paired with two embedding strategies: learnable embeddings (LE) and pretrained Word2Vec embeddings (W2V). Performance was assessed using accuracy, precision, recall, F1-score, and loss.\n",
        "\n",
        "### 2. Background\n",
        "\n",
        "#### 2.1 Model Selection\n",
        "We selected three different model architectures with increasing complexity:\n",
        "- **Logistic Regression (Baseline):** Serves as a fundamental benchmark. It transforms text into a numerical representation by averaging word embeddings and applies logistic regression for classification.\n",
        "- **Vanilla RNN:** Processes input text as a sequence of word embeddings and captures temporal dependencies through recurrent connections. However, RNNs struggle with long-term dependencies due to vanishing gradients.\n",
        "- **LSTM (Long Short-Term Memory):** An advanced form of RNN designed to handle long-range dependencies more effectively through its gating mechanisms. It is expected to outperform Vanilla RNN due to its ability to retain information across longer text sequences.\n",
        "\n",
        "#### 2.2 Word Embedding Strategies\n",
        "- **Learnable Embeddings (LE):** The model initializes an embedding layer with random weights and updates them during training. This allows the embeddings to be optimized for the specific dataset, potentially improving classification performance.\n",
        "- **Pretrained Word2Vec Embeddings (W2V):** The embeddings are trained separately on the dataset before being used in the model. This strategy helps in leveraging semantic relationships between words, reducing the risk of overfitting and improving generalization.\n",
        "\n",
        "### 3. Experiment Setup\n",
        "\n",
        "#### 3.1 Dataset Preparation\n",
        "- The **Yelp Polarity dataset** from Hugging Face was used.\n",
        "- The text was tokenized and converted into sequences of word indices.\n",
        "- Padding was applied to ensure uniform input length.\n",
        "- The dataset was split into training, validation, and test sets.\n",
        "\n",
        "#### 3.2 Model Implementation\n",
        "Each model was implemented with two embedding versions:\n",
        "- **Logistic Regression (LogReg_LE, LogReg_W2V)**\n",
        "- **Vanilla RNN (RNN_LE, RNN_W2V)**\n",
        "- **LSTM (LSTM_LE, LSTM_W2V)**\n",
        "\n",
        "All models were trained using cross-entropy loss and the Adam optimizer. Performance was evaluated using accuracy, precision, recall, and F1-score.\n",
        "\n",
        "### 4. Results and Analysis\n",
        "\n",
        "#### 4.1 Performance Metrics\n",
        "| Model        | Loss    | Accuracy | Precision | Recall  | F1-score |\n",
        "|-------------|--------|----------|-----------|---------|----------|\n",
        "| LSTM_W2V    | 0.1229 | 0.9521   | 0.9465    | 0.9584  | 0.9524   |\n",
        "| LSTM_LE     | 0.2591 | 0.9393   | 0.9280    | 0.9526  | 0.9401   |\n",
        "| LogReg_LE   | 0.2110 | 0.9312   | 0.9297    | 0.9331  | 0.9314   |\n",
        "| LogReg_W2V  | 0.2816 | 0.8946   | 0.8987    | 0.8893  | 0.8940   |\n",
        "| RNN_W2V     | 0.6258 | 0.6487   | 0.5992    | 0.8979  | 0.7188   |\n",
        "| RNN_LE      | 0.6845 | 0.5244   | 0.5266    | 0.4822  | 0.5034   |\n",
        "\n",
        "#### 4.2 Detailed Analysis\n",
        "- **LSTM_W2V achieved the highest performance** with an accuracy of **95.21%** and an F1-score of **0.9524**. This suggests that the combination of LSTM's ability to retain long-term dependencies and Word2Vecâ€™s pretrained semantic knowledge enhances sentiment classification significantly.\n",
        "- **LSTM_LE performed slightly worse** but still achieved high accuracy (93.93%) and an F1-score of 0.9401. The difference indicates that pretrained Word2Vec embeddings provided a useful semantic advantage over randomly initialized embeddings.\n",
        "- **Logistic Regression models performed well**, with **LogReg_LE** outperforming **LogReg_W2V** slightly. This suggests that for simpler models, allowing embeddings to be trained on the dataset may be more beneficial than using pretrained embeddings.\n",
        "- **RNN models performed significantly worse**, especially **RNN_LE**, which had the lowest accuracy (52.44%). This confirms that simple RNNs struggle with longer sequences, and their lack of sophisticated gating mechanisms leads to ineffective learning.\n",
        "- **RNN_W2V performed better than RNN_LE**, with a significant boost in recall (89.79%), but its lower precision indicates that it made more false positive classifications.\n",
        "\n",
        "### 5. Conclusion\n",
        "\n",
        "#### 5.1 Model Comparisons\n",
        "- **LSTM is the best-performing model overall**, demonstrating its strength in capturing long-range dependencies and handling sequential data effectively.\n",
        "- **Logistic Regression provides a strong baseline** with relatively high accuracy, making it an efficient choice when computational resources are limited.\n",
        "- **Vanilla RNN is the weakest model**, confirming that simple RNNs struggle with long text sequences and suffer from vanishing gradient problems.\n",
        "\n",
        "#### 5.2 Embedding Strategy Comparisons\n",
        "- **Word2Vec embeddings improved performance for complex models like LSTM and RNN** due to their ability to retain semantic relationships.\n",
        "- **Learnable embeddings worked better for Logistic Regression**, suggesting that training embeddings from scratch is beneficial for simpler models without sequential dependencies. However, this approach comes at the cost of significantly increased training time.\n",
        "\n",
        "#### 5.3 Final Thoughts\n",
        "- **Best Choice:** LSTM with Word2Vec (LSTM_W2V) due to its superior accuracy, F1-score, and overall robustness.\n",
        "- **Efficient Alternative:** Logistic Regression with learnable embeddings (LogReg_LE).\n",
        "- **Least Recommended:** Vanilla RNN, as it struggled significantly, especially without pretrained embeddings.\n",
        "\n",
        "This study highlights the importance of both model selection and embedding strategy in NLP tasks. Future work could explore Transformer-based models like BERT to further enhance performance.\n"
      ],
      "metadata": {
        "id": "_esltvoR7-Vd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies Setup"
      ],
      "metadata": {
        "id": "h1wTVTYwJDsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "se3YTcHf1z5o",
        "outputId": "30127e56-eb2c-4597-b285-40fafc1998ad"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "from gensim.models import Word2Vec\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertModel, BertTokenizer\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "VD9QVgQeJAC_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if using GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4F_m-9x1EMj",
        "outputId": "16d4f8b9-ee61-4c8b-ef55-158cbd53cc0e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)  # Python's random module\n",
        "np.random.seed(seed)  # NumPy's random module\n",
        "torch.manual_seed(seed)  # PyTorch's random seed for CPU\n",
        "torch.cuda.manual_seed(seed)  # PyTorch's random seed for the current GPU\n",
        "torch.cuda.manual_seed_all(seed)  # PyTorch's random seed for all GPUs (if using multi-GPU)\n",
        "\n",
        "# Ensure deterministic behavior on GPU (optional, may slow down training)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "# Optional: Set environment variables for further reproducibility\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)"
      ],
      "metadata": {
        "id": "d2fcrBPsKvXG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "### Learnable Embeddings Dataset Preprocessing\n",
        "\n"
      ],
      "metadata": {
        "id": "BWvGa6trA0nQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentDataset(Dataset):\n",
        "    \"\"\"Custom PyTorch Dataset class for sentiment analysis.\"\"\"\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = torch.tensor(sequences, dtype=torch.long)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "fJ4YzH_1yc8T"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fKKSBIkX7uH1"
      },
      "outputs": [],
      "source": [
        "class LearnableEmbeddingDataset:\n",
        "    \"\"\"Dataset preprocessing for learnable embeddings (randomly initialized nn.Embedding).\"\"\"\n",
        "\n",
        "    def __init__(self, max_seq_length=None, batch_size=32):\n",
        "        self.dataset = load_dataset(\"yelp_polarity\")  # Load dataset\n",
        "        self.tokenized_train_corpus, self.tokenized_test_corpus = self.tokenize_text()\n",
        "        self.word2idx = self.build_vocab()  # Build vocabulary (train set only)\n",
        "        self.max_seq_length = max_seq_length or self.compute_max_seq_length()\n",
        "        self.train_sequences, self.test_sequences = self.text_to_indices()\n",
        "        self.split_data()  # Train-validation split\n",
        "        self.batch_size = batch_size\n",
        "        # Create full train dataset (train + validation)\n",
        "        self.full_train_data = self.train_data + self.val_data\n",
        "        self.full_train_labels = self.train_labels + self.val_labels\n",
        "\n",
        "        # Print dataset statistics\n",
        "        print(f\" Vocabulary size: {len(self.word2idx)}\")\n",
        "        print(f\" Max sequence length: {self.max_seq_length}\")\n",
        "        print(f\" Training samples: {len(self.train_data)}\")\n",
        "        print(f\" Validation samples: {len(self.val_data)}\")\n",
        "        print(f\" Full Train Samples: {len(self.full_train_data)}\")\n",
        "        print(f\" Test samples: {len(self.test_data)}\")\n",
        "\n",
        "    def tokenize_text(self):\n",
        "        \"\"\"Tokenize text by lowercasing and splitting.\"\"\"\n",
        "        train_texts = self.dataset[\"train\"][\"text\"]\n",
        "        test_texts = self.dataset[\"test\"][\"text\"]\n",
        "        return ([text.lower().split() for text in train_texts],\n",
        "                [text.lower().split() for text in test_texts])\n",
        "\n",
        "    def build_vocab(self):\n",
        "        \"\"\"Build word-to-index mapping from training data only.\"\"\"\n",
        "        word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}  # Reserved tokens\n",
        "        idx = 2\n",
        "        for sentence in self.tokenized_train_corpus:\n",
        "            for word in sentence:\n",
        "                if word not in word2idx:\n",
        "                    word2idx[word] = idx\n",
        "                    idx += 1\n",
        "        return word2idx\n",
        "\n",
        "    def compute_max_seq_length(self):\n",
        "        \"\"\"Determine an optimal max sequence length based on training data.\"\"\"\n",
        "        train_lengths = [len(sentence) for sentence in self.tokenized_train_corpus]\n",
        "        avg_length = np.mean(train_lengths)\n",
        "        max_length = max(train_lengths)\n",
        "        computed_max_length = int(avg_length * 2)\n",
        "\n",
        "        print(f\" Average sentence length: {avg_length:.2f} words\")\n",
        "        print(f\" Longest sentence length: {max_length} words\")\n",
        "        print(f\" Computed max sequence length: {computed_max_length}\")\n",
        "\n",
        "        return computed_max_length\n",
        "\n",
        "    def text_to_indices(self):\n",
        "        \"\"\"Convert tokenized text to sequences of word indices.\"\"\"\n",
        "        def encode(sentence):\n",
        "            indices = [self.word2idx.get(word, 1) for word in sentence]  # 1 = <UNK>\n",
        "            return indices[:self.max_seq_length] + [0] * max(0, self.max_seq_length - len(indices))\n",
        "\n",
        "        train_sequences = [encode(sentence) for sentence in self.tokenized_train_corpus]\n",
        "        test_sequences = [encode(sentence) for sentence in self.tokenized_test_corpus]\n",
        "        return train_sequences, test_sequences\n",
        "\n",
        "    def split_data(self):\n",
        "        \"\"\"Split data into train, validation, and test sets.\"\"\"\n",
        "        train_labels = self.dataset[\"train\"][\"label\"]\n",
        "        test_labels = self.dataset[\"test\"][\"label\"]\n",
        "\n",
        "        self.train_data, self.val_data, self.train_labels, self.val_labels = train_test_split(\n",
        "            self.train_sequences, train_labels, test_size=0.1, random_state=42\n",
        "        )\n",
        "\n",
        "        self.test_data, self.test_labels = self.test_sequences, test_labels\n",
        "\n",
        "    def get_dataloaders(self):\n",
        "        \"\"\"Create PyTorch DataLoaders for training, validation, and testing.\"\"\"\n",
        "        train_dataset = SentimentDataset(self.train_data, self.train_labels)\n",
        "        val_dataset = SentimentDataset(self.val_data, self.val_labels)\n",
        "        test_dataset = SentimentDataset(self.test_data, self.test_labels)\n",
        "        full_train_dataset = SentimentDataset(self.full_train_data, self.full_train_labels)\n",
        "\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "        test_dataloader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "        full_train_dataloader = DataLoader(full_train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        return train_dataloader, val_dataloader, test_dataloader, full_train_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec Embeddings Dataset Preprocessing"
      ],
      "metadata": {
        "id": "D_8BKK36JLZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2VecEmbeddingDataset:\n",
        "    \"\"\"Dataset preprocessing for pretrained Word2Vec embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim=100, max_seq_length=None, batch_size=32):\n",
        "        self.dataset = load_dataset(\"yelp_polarity\")  # Load dataset\n",
        "        self.tokenized_train_corpus, self.tokenized_test_corpus = self.tokenize_text()\n",
        "        self.word2vec_model = self.train_word2vec(embedding_dim)  # Train Word2Vec\n",
        "        self.word2idx = self.rebuild_word2idx()  # Rebuild vocabulary to match Word2Vec order\n",
        "        self.max_seq_length = max_seq_length or self.compute_max_seq_length()\n",
        "        self.train_sequences, self.test_sequences = self.text_to_indices()\n",
        "        self.split_data()  # Train-validation split\n",
        "        self.batch_size = batch_size\n",
        "        self.embedding_matrix = self.build_embedding_matrix()  # Convert Word2Vec embeddings to PyTorch tensor\n",
        "        # Create full train dataset (train + validation)\n",
        "        self.full_train_data = self.train_data + self.val_data\n",
        "        self.full_train_labels = self.train_labels + self.val_labels\n",
        "\n",
        "        # Print dataset statistics\n",
        "        print(f\" Vocabulary size: {len(self.word2idx)}\")\n",
        "        print(f\" Max sequence length: {self.max_seq_length}\")\n",
        "        print(f\" Training samples: {len(self.train_data)}\")\n",
        "        print(f\" Validation samples: {len(self.val_data)}\")\n",
        "        print(f\" Full Train Samples: {len(self.full_train_data)}\")\n",
        "        print(f\" Test samples: {len(self.test_data)}\")\n",
        "        print(f\" Embedding matrix shape: {self.embedding_matrix.shape}\")\n",
        "\n",
        "    def tokenize_text(self):\n",
        "        \"\"\"Tokenize text by lowercasing and splitting.\"\"\"\n",
        "        train_texts = self.dataset[\"train\"][\"text\"]\n",
        "        test_texts = self.dataset[\"test\"][\"text\"]\n",
        "        return ([text.lower().split() for text in train_texts],\n",
        "                [text.lower().split() for text in test_texts])\n",
        "\n",
        "    def train_word2vec(self, embedding_dim):\n",
        "        \"\"\"Train Word2Vec on training data only.\"\"\"\n",
        "        return Word2Vec(sentences=self.tokenized_train_corpus, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
        "\n",
        "    def rebuild_word2idx(self):\n",
        "        \"\"\"Rebuild word2idx to match Word2Vec order.\"\"\"\n",
        "        word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "        for idx, word in enumerate(self.word2vec_model.wv.index_to_key, start=2):\n",
        "            word2idx[word] = idx\n",
        "        return word2idx\n",
        "\n",
        "    def compute_max_seq_length(self):\n",
        "        \"\"\"Determine an optimal max sequence length based on training data.\"\"\"\n",
        "        train_lengths = [len(sentence) for sentence in self.tokenized_train_corpus]\n",
        "        avg_length = np.mean(train_lengths)\n",
        "        max_length = max(train_lengths)\n",
        "        computed_max_length = int(avg_length * 2)\n",
        "\n",
        "        print(f\" Average sentence length: {avg_length:.2f} words\")\n",
        "        print(f\" Longest sentence length: {max_length} words\")\n",
        "        print(f\" Computed max sequence length: {computed_max_length}\")\n",
        "\n",
        "        return computed_max_length\n",
        "\n",
        "    def text_to_indices(self):\n",
        "        \"\"\"Convert tokenized text to sequences of word indices.\"\"\"\n",
        "        def encode(sentence):\n",
        "            indices = [self.word2idx.get(word, 1) for word in sentence]  # 1 = <UNK>\n",
        "            return indices[:self.max_seq_length] + [0] * max(0, self.max_seq_length - len(indices))\n",
        "\n",
        "        train_sequences = [encode(sentence) for sentence in self.tokenized_train_corpus]\n",
        "        test_sequences = [encode(sentence) for sentence in self.tokenized_test_corpus]\n",
        "        return train_sequences, test_sequences\n",
        "\n",
        "    def build_embedding_matrix(self):\n",
        "        \"\"\"Return the PyTorch tensor of the Word2Vec embeddings, including <PAD> and <UNK>.\"\"\"\n",
        "        word_embeddings = torch.FloatTensor(self.word2vec_model.wv.vectors)\n",
        "\n",
        "        # Manually add <PAD> and <UNK> embeddings\n",
        "        pad_embedding = torch.zeros((1, self.word2vec_model.vector_size))  # Zero vector for padding\n",
        "        unk_embedding = torch.mean(word_embeddings, dim=0, keepdim=True)  # Average embedding for <UNK>\n",
        "\n",
        "        # Concatenate <PAD> and <UNK> embeddings at the beginning\n",
        "        word_embeddings = torch.cat([pad_embedding, unk_embedding, word_embeddings], dim=0)\n",
        "\n",
        "        return word_embeddings\n",
        "\n",
        "    def get_embedding_matrix(self):\n",
        "        \"\"\"Return the PyTorch tensor of the Word2Vec embeddings, including <PAD> and <UNK>.\"\"\"\n",
        "        return self.embedding_matrix\n",
        "\n",
        "    def split_data(self):\n",
        "        \"\"\"Split data into train, validation, and test sets.\"\"\"\n",
        "        train_labels = self.dataset[\"train\"][\"label\"]\n",
        "        test_labels = self.dataset[\"test\"][\"label\"]\n",
        "\n",
        "        self.train_data, self.val_data, self.train_labels, self.val_labels = train_test_split(\n",
        "            self.train_sequences, train_labels, test_size=0.1, random_state=42\n",
        "        )\n",
        "\n",
        "        self.test_data, self.test_labels = self.test_sequences, test_labels\n",
        "\n",
        "    def get_dataloaders(self):\n",
        "        \"\"\"Create PyTorch DataLoaders for training, validation, and testing.\"\"\"\n",
        "        train_dataset = SentimentDataset(self.train_data, self.train_labels)\n",
        "        val_dataset = SentimentDataset(self.val_data, self.val_labels)\n",
        "        test_dataset = SentimentDataset(self.test_data, self.test_labels)\n",
        "        full_train_dataset = SentimentDataset(self.full_train_data, self.full_train_labels)\n",
        "\n",
        "        train_dataloader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        val_dataloader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "        test_dataloader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "        full_train_dataloader = DataLoader(full_train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "\n",
        "        return train_dataloader, val_dataloader, test_dataloader, full_train_dataloader"
      ],
      "metadata": {
        "id": "igxhcdftJc-h"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to test dataset module and verify Word2Vec embeddings\n",
        "def test_dataset_module(dataset_class, embedding_type, batch_size=32, embedding_dim=None):\n",
        "    \"\"\"Tests the given dataset module by printing dataset statistics and a sample batch.\"\"\"\n",
        "    print(f\" Testing {embedding_type} Dataset with batch size {batch_size}...\")\n",
        "\n",
        "    # Instantiate dataset module (handle cases where embedding_dim is needed)\n",
        "    dataset = dataset_class(embedding_dim=embedding_dim, max_seq_length=None, batch_size=batch_size) if embedding_dim else dataset_class(max_seq_length=None, batch_size=batch_size)\n",
        "\n",
        "    # Print dataset statistics\n",
        "    print(f\"   {embedding_type} Dataset Stats:\")\n",
        "    print(f\"   - Vocabulary size: {len(dataset.word2idx)}\")\n",
        "    print(f\"   - Max sequence length: {dataset.max_seq_length}\")\n",
        "    print(f\"   - Training samples: {len(dataset.train_data)}\")\n",
        "    print(f\"   - Validation samples: {len(dataset.val_data)}\")\n",
        "    full_train_count = len(dataset.full_train_data) if hasattr(dataset, \"full_train_data\") else \"N/A\"\n",
        "    print(f\"   - Full Train Samples: {full_train_count}\")\n",
        "    print(f\"   - Test samples: {len(dataset.test_data)}\")\n",
        "\n",
        "    # Sentence length stats (from compute_max_seq_length)\n",
        "    avg_length = np.mean([len(sentence) for sentence in dataset.tokenized_train_corpus])\n",
        "    max_length = max(len(sentence) for sentence in dataset.tokenized_train_corpus)\n",
        "    print(f\"   - Average sentence length: {avg_length:.2f} words\")\n",
        "    print(f\"   - Longest sentence length: {max_length} words\")\n",
        "    print(f\"   - Computed max sequence length: {dataset.compute_max_seq_length()}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Get DataLoaders\n",
        "    train_dataloader, val_dataloader, test_dataloader, full_train_dataloader = dataset.get_dataloaders()\n",
        "\n",
        "    # Retrieve a batch\n",
        "    train_batch = next(iter(train_dataloader))\n",
        "    print(f\" {embedding_type} Batch Shape: {train_batch[0].shape}, Labels Shape: {train_batch[1].shape}\")\n",
        "\n",
        "    # If testing Word2Vec embeddings, retrieve and verify the embedding matrix\n",
        "    if isinstance(dataset, Word2VecEmbeddingDataset):\n",
        "        print(\" Testing Word2Vec Embedding Matrix...\")\n",
        "        embedding_matrix = dataset.get_embedding_matrix()\n",
        "\n",
        "        # Print embedding matrix shape\n",
        "        print(f\" Embedding Matrix Shape: {embedding_matrix.shape}\")  # Should match (vocab_size, embedding_dim)\n",
        "\n",
        "        # Check special token embeddings\n",
        "        pad_embedding = embedding_matrix[0]  # First row should be all zeros\n",
        "        unk_embedding = embedding_matrix[1]  # Second row should be the average of all embeddings\n",
        "\n",
        "        print(f\" <PAD> Embedding (should be all zeros): {pad_embedding[:5]} ...\")  # Print first few values\n",
        "        print(f\" <UNK> Embedding (should be avg of embeddings): {unk_embedding[:5]} ...\")  # Print first few values\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "\n",
        "# Run tests for Learnable and Word2Vec Embeddings\n",
        "test_dataset_module(LearnableEmbeddingDataset, \"Learnable Embeddings\", batch_size=32)\n",
        "test_dataset_module(Word2VecEmbeddingDataset, \"Word2Vec Embeddings\", batch_size=32, embedding_dim=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZ-2UrcaLNQi",
        "outputId": "85871f95-96cc-48d1-f974-07f81957eb4d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Testing Learnable Embeddings Dataset with batch size 32...\n",
            " Average sentence length: 133.03 words\n",
            " Longest sentence length: 1052 words\n",
            " Computed max sequence length: 266\n",
            " Vocabulary size: 1288540\n",
            " Max sequence length: 266\n",
            " Training samples: 504000\n",
            " Validation samples: 56000\n",
            " Full Train Samples: 560000\n",
            " Test samples: 38000\n",
            "   Learnable Embeddings Dataset Stats:\n",
            "   - Vocabulary size: 1288540\n",
            "   - Max sequence length: 266\n",
            "   - Training samples: 504000\n",
            "   - Validation samples: 56000\n",
            "   - Full Train Samples: 560000\n",
            "   - Test samples: 38000\n",
            "   - Average sentence length: 133.03 words\n",
            "   - Longest sentence length: 1052 words\n",
            " Average sentence length: 133.03 words\n",
            " Longest sentence length: 1052 words\n",
            " Computed max sequence length: 266\n",
            "   - Computed max sequence length: 266\n",
            "--------------------------------------------------\n",
            " Learnable Embeddings Batch Shape: torch.Size([32, 266]), Labels Shape: torch.Size([32])\n",
            "==================================================\n",
            " Testing Word2Vec Embeddings Dataset with batch size 32...\n",
            " Average sentence length: 133.03 words\n",
            " Longest sentence length: 1052 words\n",
            " Computed max sequence length: 266\n",
            " Vocabulary size: 1288540\n",
            " Max sequence length: 266\n",
            " Training samples: 504000\n",
            " Validation samples: 56000\n",
            " Full Train Samples: 560000\n",
            " Test samples: 38000\n",
            " Embedding matrix shape: torch.Size([1288540, 100])\n",
            "   Word2Vec Embeddings Dataset Stats:\n",
            "   - Vocabulary size: 1288540\n",
            "   - Max sequence length: 266\n",
            "   - Training samples: 504000\n",
            "   - Validation samples: 56000\n",
            "   - Full Train Samples: 560000\n",
            "   - Test samples: 38000\n",
            "   - Average sentence length: 133.03 words\n",
            "   - Longest sentence length: 1052 words\n",
            " Average sentence length: 133.03 words\n",
            " Longest sentence length: 1052 words\n",
            " Computed max sequence length: 266\n",
            "   - Computed max sequence length: 266\n",
            "--------------------------------------------------\n",
            " Word2Vec Embeddings Batch Shape: torch.Size([32, 266]), Labels Shape: torch.Size([32])\n",
            " Testing Word2Vec Embedding Matrix...\n",
            " Embedding Matrix Shape: torch.Size([1288540, 100])\n",
            " <PAD> Embedding (should be all zeros): tensor([0., 0., 0., 0., 0.]) ...\n",
            " <UNK> Embedding (should be avg of embeddings): tensor([-0.0128,  0.0295, -0.0064, -0.0045,  0.0111]) ...\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate dataset objects\n",
        "le_dataset = LearnableEmbeddingDataset()\n",
        "w2v_dataset = Word2VecEmbeddingDataset()\n",
        "\n",
        "# Get the DataLoaders\n",
        "train_dataloader_LE, val_dataloader_LE, test_dataloader_LE, full_train_dataloader_LE = le_dataset.get_dataloaders()\n",
        "train_dataloader_Word2Vec, val_dataloader_Word2Vec, test_dataloader_Word2Vec, full_train_dataloader_Word2Vec= w2v_dataset.get_dataloaders()\n",
        "\n",
        "# Get the Word2Vec embedding matrix (for models using pretrained embeddings)\n",
        "word2vec_embeddings = w2v_dataset.get_embedding_matrix()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjGDkU3O5cCN",
        "outputId": "a0d231ea-8c43-4cd0-f631-f45c5ba28f04"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Average sentence length: 133.03 words\n",
            " Longest sentence length: 1052 words\n",
            " Computed max sequence length: 266\n",
            " Vocabulary size: 1288540\n",
            " Max sequence length: 266\n",
            " Training samples: 504000\n",
            " Validation samples: 56000\n",
            " Full Train Samples: 560000\n",
            " Test samples: 38000\n",
            " Average sentence length: 133.03 words\n",
            " Longest sentence length: 1052 words\n",
            " Computed max sequence length: 266\n",
            " Vocabulary size: 1288540\n",
            " Max sequence length: 266\n",
            " Training samples: 504000\n",
            " Validation samples: 56000\n",
            " Full Train Samples: 560000\n",
            " Test samples: 38000\n",
            " Embedding matrix shape: torch.Size([1288540, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity Check to ensure no data leakage\n",
        "def count_oov_words(dataset):\n",
        "    \"\"\"\n",
        "    Count the number of out-of-vocabulary (OOV) words in the test set.\n",
        "\n",
        "    Args:\n",
        "        dataset: The dataset object (LearnableEmbeddingDataset or Word2VecEmbeddingDataset).\n",
        "\n",
        "    Returns:\n",
        "        oov_count (int): Total number of OOV words.\n",
        "        oov_percentage (float): Percentage of OOV words in test set.\n",
        "    \"\"\"\n",
        "    oov_count = 0\n",
        "    total_words = 0\n",
        "\n",
        "    for sentence in dataset.tokenized_test_corpus:\n",
        "        for word in sentence:\n",
        "            total_words += 1\n",
        "            if word not in dataset.word2idx:  # Word not in vocabulary\n",
        "                oov_count += 1\n",
        "\n",
        "    oov_percentage = (oov_count / total_words) * 100\n",
        "    print(f\"\\n OOV Words in Test Set: {oov_count}/{total_words} ({oov_percentage:.2f}%)\")\n",
        "    return oov_count, oov_percentage"
      ],
      "metadata": {
        "id": "BjWDD29bcthM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check OOV rate for Learnable Embeddings\n",
        "print(\"\\n OOV Words for Learnable Embeddings:\")\n",
        "oov_LE, oov_LE_percentage = count_oov_words(le_dataset)\n",
        "\n",
        "# Check OOV rate for Word2Vec Embeddings\n",
        "print(\"\\n OOV Words for Word2Vec Embeddings:\")\n",
        "oov_W2V, oov_W2V_percentage = count_oov_words(w2v_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8RKs_j1c3mG",
        "outputId": "3946c7b9-36a1-4e0c-cf1e-d4fb26f12c15"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " OOV Words for Learnable Embeddings:\n",
            "\n",
            " OOV Words in Test Set: 63627/5037228 (1.26%)\n",
            "\n",
            " OOV Words for Word2Vec Embeddings:\n",
            "\n",
            " OOV Words in Test Set: 63627/5037228 (1.26%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Implementation\n",
        "\n"
      ],
      "metadata": {
        "id": "Oap0nZxXtVQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "cqXW1Czytxee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionModel(nn.Module):\n",
        "    \"\"\"Baseline Logistic Regression model using averaged word embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, pretrained_embeddings=None, train_embeddings=True):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "\n",
        "        if pretrained_embeddings is not None:\n",
        "            self.embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=not train_embeddings)\n",
        "        else:\n",
        "            self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.fc = nn.Linear(embedding_dim, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embeddings(x)  # Shape: (batch_size, seq_len, embedding_dim)\n",
        "        avg_embedding = embedded.mean(dim=1)  # Average over sequence length\n",
        "        output = self.fc(avg_embedding)  # Fully connected layer\n",
        "        return self.sigmoid(output).squeeze()"
      ],
      "metadata": {
        "id": "AY6wHWkttUbX"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RNN"
      ],
      "metadata": {
        "id": "2UWTYC12vL1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "    \"\"\"RNN-based sentiment classification model.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, pretrained_embeddings=None, train_embeddings=True):\n",
        "        super(RNNModel, self).__init__()\n",
        "\n",
        "        if pretrained_embeddings is not None:\n",
        "            self.embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=not train_embeddings)\n",
        "        else:\n",
        "            self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embeddings(x)\n",
        "        rnn_out, _ = self.rnn(embedded)\n",
        "        last_hidden_state = rnn_out[:, -1, :]  # Take last hidden state\n",
        "        output = self.fc(last_hidden_state)\n",
        "        return self.sigmoid(output).squeeze()"
      ],
      "metadata": {
        "id": "mvX3QxhHvHIh"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM"
      ],
      "metadata": {
        "id": "xnNYAQIxvPe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"LSTM-based sentiment classification model.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, pretrained_embeddings=None, train_embeddings=True):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        if pretrained_embeddings is not None:\n",
        "            self.embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=not train_embeddings)\n",
        "        else:\n",
        "            self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embeddings(x)\n",
        "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
        "        last_hidden_state = hidden[-1]  # Take last hidden state\n",
        "        output = self.fc(last_hidden_state)\n",
        "        return self.sigmoid(output).squeeze()"
      ],
      "metadata": {
        "id": "vE_wp2cwvOoH"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training & Evaluation Pipeline"
      ],
      "metadata": {
        "id": "WCej69NM5rZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    \"\"\"Reusable training and evaluation class for all models.\"\"\"\n",
        "\n",
        "    def __init__(self, model, train_dataloader, val_dataloader, test_dataloader,\n",
        "                 full_train_dataloader=None, learning_rate=0.001, num_epochs=10,\n",
        "                 device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n",
        "        self.model = model.to(device)\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.test_dataloader = test_dataloader\n",
        "        self.full_train_dataloader = full_train_dataloader  # Store full dataset loader for retraining\n",
        "        self.criterion = nn.BCELoss()  # Binary classification loss\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
        "        self.num_epochs = num_epochs\n",
        "        self.device = device\n",
        "\n",
        "    def train_model(self, use_full_data=False):\n",
        "        \"\"\"Train the model with either train data or full (train + validation) data.\"\"\"\n",
        "        dataloader = self.full_train_dataloader if use_full_data else self.train_dataloader\n",
        "        data_type = \"Full Train (Train+Val)\" if use_full_data else \"Train Only\"\n",
        "\n",
        "        print(f\"\\n Training {self.model.__class__.__name__} on {data_type} for {self.num_epochs} epochs...\\n\")\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            self.model.train()  # Set model to training mode\n",
        "            total_loss = 0\n",
        "            all_preds, all_labels = [], []\n",
        "\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()  # Reset gradients\n",
        "                outputs = self.model(inputs).squeeze()  # Ensure correct shape\n",
        "\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                loss.backward()  # Compute gradients\n",
        "                self.optimizer.step()  # Update weights\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                # Convert outputs to binary predictions\n",
        "                predictions = (outputs >= 0.5).float()\n",
        "                all_preds.extend(predictions.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "            # Compute Training Accuracy\n",
        "            train_accuracy = accuracy_score(all_labels, all_preds)\n",
        "            avg_train_loss = total_loss / len(dataloader)\n",
        "\n",
        "            # Evaluate on Validation Set (only when not using full data)\n",
        "            if not use_full_data:\n",
        "                val_metrics = self.evaluate_model(self.val_dataloader, mode=\"Validation\")\n",
        "                print(f\"Epoch {epoch+1}/{self.num_epochs} | \"\n",
        "                      f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f} | \"\n",
        "                      f\"Val Loss: {val_metrics['loss']:.4f} | Val Acc: {val_metrics['accuracy']:.4f} | \"\n",
        "                      f\"Val F1: {val_metrics['f1-score']:.4f} | \"\n",
        "                      f\"Val Precision: {val_metrics['precision']:.4f} | \"\n",
        "                      f\"Val Recall: {val_metrics['recall']:.4f}\")\n",
        "            else:\n",
        "                print(f\"Epoch {epoch+1}/{self.num_epochs} | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_accuracy:.4f}\")\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "        print(f\"\\n Training completed in {total_time:.2f} seconds.\")\n",
        "\n",
        "    def evaluate_model(self, dataloader, mode=\"Test\"):\n",
        "        \"\"\"Evaluate the model on a given dataset (validation or test).\"\"\"\n",
        "        self.model.eval()  # Set model to evaluation mode\n",
        "        all_preds, all_labels = [], []\n",
        "        total_loss = 0\n",
        "\n",
        "        with torch.no_grad():  # No gradient computation in evaluation\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                outputs = self.model(inputs).squeeze()\n",
        "\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "                predictions = (outputs >= 0.5).float()  # Convert logits to binary predictions\n",
        "\n",
        "                all_preds.extend(predictions.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        acc = accuracy_score(all_labels, all_preds)\n",
        "        precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_preds, average=\"binary\", zero_division=0)\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "\n",
        "        # Print Classification Report\n",
        "        print(f\"\\n {mode} Classification Report:\\n\", classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "        return {\"loss\": avg_loss, \"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1-score\": f1}\n",
        "\n",
        "    def retrain_and_test(self):\n",
        "        \"\"\"Retrain model using both train & validation sets, then test on test set.\"\"\"\n",
        "        if self.full_train_dataloader is None:\n",
        "            print(\"Full train dataloader is missing! Ensure it's passed during initialization.\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\n Retraining {self.model.__class__.__name__} using Train + Validation Data...\\n\")\n",
        "        self.train_model(use_full_data=True)  # Train the model again with full dataset\n",
        "\n",
        "        print(f\"\\n Final Testing on Test Set...\")\n",
        "        test_metrics = self.evaluate_model(self.test_dataloader, mode=\"Test\")\n",
        "\n",
        "        print(f\"Final Test Results: Accuracy: {test_metrics['accuracy']:.4f}, Loss: {test_metrics['loss']:.4f} | \"\n",
        "              f\"Precision: {test_metrics['precision']:.4f} | \"\n",
        "              f\"Recall: {test_metrics['recall']:.4f} | \"\n",
        "              f\"F1-score: {test_metrics['f1-score']:.4f}\")"
      ],
      "metadata": {
        "id": "H425CY8554Z3"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment"
      ],
      "metadata": {
        "id": "rhJLKjEIX2uj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define model hyperparameters\n",
        "embedding_dim = 100  # Same as Word2Vec embedding size\n",
        "hidden_size = 128  # For RNN/LSTM\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Get vocabulary sizes\n",
        "vocab_size_LE = len(le_dataset.word2idx)  # Learnable embeddings vocabulary size\n",
        "vocab_size_W2V = len(w2v_dataset.word2idx)  # Word2Vec embeddings vocabulary size\n",
        "\n",
        "# Define all models with both embedding strategies\n",
        "models = {\n",
        "    \"LogReg_LE\": LogisticRegressionModel(vocab_size_LE, embedding_dim),\n",
        "    \"LogReg_W2V\": LogisticRegressionModel(vocab_size_W2V, embedding_dim, pretrained_embeddings=word2vec_embeddings, train_embeddings=False),\n",
        "\n",
        "    \"RNN_LE\": RNNModel(vocab_size_LE, embedding_dim, hidden_size),\n",
        "    \"RNN_W2V\": RNNModel(vocab_size_W2V, embedding_dim, hidden_size, pretrained_embeddings=word2vec_embeddings, train_embeddings=False),\n",
        "\n",
        "    \"LSTM_LE\": LSTMModel(vocab_size_LE, embedding_dim, hidden_size),\n",
        "    \"LSTM_W2V\": LSTMModel(vocab_size_W2V, embedding_dim, hidden_size, pretrained_embeddings=word2vec_embeddings, train_embeddings=False)\n",
        "}\n",
        "\n",
        "# Define dataloaders for both strategies\n",
        "dataloaders = {\n",
        "    \"LE\": (train_dataloader_LE, val_dataloader_LE, test_dataloader_LE, full_train_dataloader_LE),\n",
        "    \"W2V\": (train_dataloader_Word2Vec, val_dataloader_Word2Vec, test_dataloader_Word2Vec, full_train_dataloader_Word2Vec)\n",
        "}\n",
        "\n",
        "# Train & Evaluate all models\n",
        "results = {}\n",
        "\n",
        "# Train, evaluate, and retrain each model\n",
        "for model_name, model in models.items():\n",
        "\n",
        "    # Add big seperation line\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    print(f\"\\n Training {model_name}...\\n\")\n",
        "\n",
        "    # Determine which dataloaders to use\n",
        "    strategy = \"LE\" if \"LE\" in model_name else \"W2V\"\n",
        "    train_dl, val_dl, test_dl, full_train_dl = dataloaders[strategy]\n",
        "\n",
        "    # Initialize trainer\n",
        "    trainer = Trainer(model, train_dl, val_dl, test_dl, full_train_dataloader=full_train_dl,\n",
        "                      learning_rate=learning_rate, num_epochs=num_epochs)\n",
        "\n",
        "    # Train and evaluate model\n",
        "    trainer.train_model()\n",
        "\n",
        "    # Add small seperation line\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Retrain on full train data and test\n",
        "    trainer.retrain_and_test()\n",
        "\n",
        "    # Add seperation line\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # Store results\n",
        "    test_metrics = trainer.evaluate_model(trainer.test_dataloader, mode=\"Test\")\n",
        "    results[model_name] = test_metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9QZP-8tX_wy",
        "outputId": "104f5976-c8cf-4f83-e26e-3f3f6e39f9eb"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            " Training LogReg_LE...\n",
            "\n",
            "\n",
            " Training LogisticRegressionModel on Train Only for 5 epochs...\n",
            "\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9358    0.9104    0.9229     28035\n",
            "         1.0     0.9126    0.9374    0.9248     27965\n",
            "\n",
            "    accuracy                         0.9239     56000\n",
            "   macro avg     0.9242    0.9239    0.9239     56000\n",
            "weighted avg     0.9242    0.9239    0.9239     56000\n",
            "\n",
            "Epoch 1/5 | Train Loss: 0.2869 | Train Acc: 0.8880 | Val Loss: 0.2093 | Val Acc: 0.9239 | Val F1: 0.9248 | Val Precision: 0.9126 | Val Recall: 0.9374\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9415    0.9149    0.9280     28035\n",
            "         1.0     0.9171    0.9430    0.9299     27965\n",
            "\n",
            "    accuracy                         0.9290     56000\n",
            "   macro avg     0.9293    0.9290    0.9290     56000\n",
            "weighted avg     0.9293    0.9290    0.9290     56000\n",
            "\n",
            "Epoch 2/5 | Train Loss: 0.1850 | Train Acc: 0.9337 | Val Loss: 0.1943 | Val Acc: 0.9290 | Val F1: 0.9299 | Val Precision: 0.9171 | Val Recall: 0.9430\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9397    0.9227    0.9311     28035\n",
            "         1.0     0.9239    0.9406    0.9322     27965\n",
            "\n",
            "    accuracy                         0.9316     56000\n",
            "   macro avg     0.9318    0.9317    0.9316     56000\n",
            "weighted avg     0.9318    0.9316    0.9316     56000\n",
            "\n",
            "Epoch 3/5 | Train Loss: 0.1626 | Train Acc: 0.9424 | Val Loss: 0.1898 | Val Acc: 0.9316 | Val F1: 0.9322 | Val Precision: 0.9239 | Val Recall: 0.9406\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9407    0.9183    0.9294     28035\n",
            "         1.0     0.9200    0.9420    0.9308     27965\n",
            "\n",
            "    accuracy                         0.9301     56000\n",
            "   macro avg     0.9303    0.9301    0.9301     56000\n",
            "weighted avg     0.9304    0.9301    0.9301     56000\n",
            "\n",
            "Epoch 4/5 | Train Loss: 0.1471 | Train Acc: 0.9483 | Val Loss: 0.1918 | Val Acc: 0.9301 | Val F1: 0.9308 | Val Precision: 0.9200 | Val Recall: 0.9420\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9448    0.9144    0.9293     28035\n",
            "         1.0     0.9168    0.9465    0.9314     27965\n",
            "\n",
            "    accuracy                         0.9304     56000\n",
            "   macro avg     0.9308    0.9304    0.9304     56000\n",
            "weighted avg     0.9308    0.9304    0.9304     56000\n",
            "\n",
            "Epoch 5/5 | Train Loss: 0.1352 | Train Acc: 0.9530 | Val Loss: 0.1935 | Val Acc: 0.9304 | Val F1: 0.9314 | Val Precision: 0.9168 | Val Recall: 0.9465\n",
            "\n",
            " Training completed in 3412.25 seconds.\n",
            "--------------------------------------------------\n",
            "\n",
            " Retraining LogisticRegressionModel using Train + Validation Data...\n",
            "\n",
            "\n",
            " Training LogisticRegressionModel on Full Train (Train+Val) for 5 epochs...\n",
            "\n",
            "Epoch 1/5 | Train Loss: 0.1321 | Train Acc: 0.9542\n",
            "Epoch 2/5 | Train Loss: 0.1227 | Train Acc: 0.9578\n",
            "Epoch 3/5 | Train Loss: 0.1148 | Train Acc: 0.9607\n",
            "Epoch 4/5 | Train Loss: 0.1077 | Train Acc: 0.9633\n",
            "Epoch 5/5 | Train Loss: 0.1016 | Train Acc: 0.9652\n",
            "\n",
            " Training completed in 3800.54 seconds.\n",
            "\n",
            " Final Testing on Test Set...\n",
            "\n",
            " Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9328    0.9294    0.9311     19000\n",
            "         1.0     0.9297    0.9331    0.9314     19000\n",
            "\n",
            "    accuracy                         0.9312     38000\n",
            "   macro avg     0.9312    0.9312    0.9312     38000\n",
            "weighted avg     0.9312    0.9312    0.9312     38000\n",
            "\n",
            "Final Test Results: Accuracy: 0.9312, Loss: 0.2110 | Precision: 0.9297 | Recall: 0.9331 | F1-score: 0.9314\n",
            "--------------------------------------------------\n",
            "\n",
            " Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9328    0.9294    0.9311     19000\n",
            "         1.0     0.9297    0.9331    0.9314     19000\n",
            "\n",
            "    accuracy                         0.9312     38000\n",
            "   macro avg     0.9312    0.9312    0.9312     38000\n",
            "weighted avg     0.9312    0.9312    0.9312     38000\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            " Training LogReg_W2V...\n",
            "\n",
            "\n",
            " Training LogisticRegressionModel on Train Only for 5 epochs...\n",
            "\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.8909    0.8708    0.8807     28035\n",
            "         1.0     0.8733    0.8931    0.8831     27965\n",
            "\n",
            "    accuracy                         0.8819     56000\n",
            "   macro avg     0.8821    0.8820    0.8819     56000\n",
            "weighted avg     0.8821    0.8819    0.8819     56000\n",
            "\n",
            "Epoch 1/5 | Train Loss: 0.3691 | Train Acc: 0.8625 | Val Loss: 0.3177 | Val Acc: 0.8819 | Val F1: 0.8831 | Val Precision: 0.8733 | Val Recall: 0.8931\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.8858    0.8916    0.8887     28035\n",
            "         1.0     0.8906    0.8847    0.8877     27965\n",
            "\n",
            "    accuracy                         0.8882     56000\n",
            "   macro avg     0.8882    0.8882    0.8882     56000\n",
            "weighted avg     0.8882    0.8882    0.8882     56000\n",
            "\n",
            "Epoch 2/5 | Train Loss: 0.3073 | Train Acc: 0.8860 | Val Loss: 0.3014 | Val Acc: 0.8882 | Val F1: 0.8877 | Val Precision: 0.8906 | Val Recall: 0.8847\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.8946    0.8835    0.8890     28035\n",
            "         1.0     0.8846    0.8957    0.8901     27965\n",
            "\n",
            "    accuracy                         0.8896     56000\n",
            "   macro avg     0.8896    0.8896    0.8896     56000\n",
            "weighted avg     0.8896    0.8896    0.8896     56000\n",
            "\n",
            "Epoch 3/5 | Train Loss: 0.2966 | Train Acc: 0.8898 | Val Loss: 0.2945 | Val Acc: 0.8896 | Val F1: 0.8901 | Val Precision: 0.8846 | Val Recall: 0.8957\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.8934    0.8886    0.8910     28035\n",
            "         1.0     0.8890    0.8937    0.8913     27965\n",
            "\n",
            "    accuracy                         0.8912     56000\n",
            "   macro avg     0.8912    0.8912    0.8912     56000\n",
            "weighted avg     0.8912    0.8912    0.8912     56000\n",
            "\n",
            "Epoch 4/5 | Train Loss: 0.2920 | Train Acc: 0.8910 | Val Loss: 0.2913 | Val Acc: 0.8912 | Val F1: 0.8913 | Val Precision: 0.8890 | Val Recall: 0.8937\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.8955    0.8866    0.8910     28035\n",
            "         1.0     0.8874    0.8962    0.8918     27965\n",
            "\n",
            "    accuracy                         0.8914     56000\n",
            "   macro avg     0.8914    0.8914    0.8914     56000\n",
            "weighted avg     0.8915    0.8914    0.8914     56000\n",
            "\n",
            "Epoch 5/5 | Train Loss: 0.2895 | Train Acc: 0.8917 | Val Loss: 0.2894 | Val Acc: 0.8914 | Val F1: 0.8918 | Val Precision: 0.8874 | Val Recall: 0.8962\n",
            "\n",
            " Training completed in 148.47 seconds.\n",
            "--------------------------------------------------\n",
            "\n",
            " Retraining LogisticRegressionModel using Train + Validation Data...\n",
            "\n",
            "\n",
            " Training LogisticRegressionModel on Full Train (Train+Val) for 5 epochs...\n",
            "\n",
            "Epoch 1/5 | Train Loss: 0.2881 | Train Acc: 0.8922\n",
            "Epoch 2/5 | Train Loss: 0.2871 | Train Acc: 0.8925\n",
            "Epoch 3/5 | Train Loss: 0.2865 | Train Acc: 0.8926\n",
            "Epoch 4/5 | Train Loss: 0.2861 | Train Acc: 0.8926\n",
            "Epoch 5/5 | Train Loss: 0.2859 | Train Acc: 0.8929\n",
            "\n",
            " Training completed in 156.29 seconds.\n",
            "\n",
            " Final Testing on Test Set...\n",
            "\n",
            " Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.8905    0.8998    0.8951     19000\n",
            "         1.0     0.8987    0.8893    0.8940     19000\n",
            "\n",
            "    accuracy                         0.8946     38000\n",
            "   macro avg     0.8946    0.8946    0.8945     38000\n",
            "weighted avg     0.8946    0.8946    0.8945     38000\n",
            "\n",
            "Final Test Results: Accuracy: 0.8946, Loss: 0.2816 | Precision: 0.8987 | Recall: 0.8893 | F1-score: 0.8940\n",
            "--------------------------------------------------\n",
            "\n",
            " Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.8905    0.8998    0.8951     19000\n",
            "         1.0     0.8987    0.8893    0.8940     19000\n",
            "\n",
            "    accuracy                         0.8946     38000\n",
            "   macro avg     0.8946    0.8946    0.8945     38000\n",
            "weighted avg     0.8946    0.8946    0.8945     38000\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            " Training RNN_LE...\n",
            "\n",
            "\n",
            " Training RNNModel on Train Only for 5 epochs...\n",
            "\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.6923    0.1377    0.2297     28035\n",
            "         1.0     0.5206    0.9386    0.6697     27965\n",
            "\n",
            "    accuracy                         0.5377     56000\n",
            "   macro avg     0.6064    0.5382    0.4497     56000\n",
            "weighted avg     0.6065    0.5377    0.4494     56000\n",
            "\n",
            "Epoch 1/5 | Train Loss: 0.6870 | Train Acc: 0.5281 | Val Loss: 0.6850 | Val Acc: 0.5377 | Val F1: 0.6697 | Val Precision: 0.5206 | Val Recall: 0.9386\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.6889    0.1389    0.2312     28035\n",
            "         1.0     0.5205    0.9371    0.6693     27965\n",
            "\n",
            "    accuracy                         0.5375     56000\n",
            "   macro avg     0.6047    0.5380    0.4503     56000\n",
            "weighted avg     0.6048    0.5375    0.4500     56000\n",
            "\n",
            "Epoch 2/5 | Train Loss: 0.6853 | Train Acc: 0.5281 | Val Loss: 0.6843 | Val Acc: 0.5375 | Val F1: 0.6693 | Val Precision: 0.5205 | Val Recall: 0.9371\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7261    0.7705    0.7476     28035\n",
            "         1.0     0.7549    0.7086    0.7310     27965\n",
            "\n",
            "    accuracy                         0.7396     56000\n",
            "   macro avg     0.7405    0.7395    0.7393     56000\n",
            "weighted avg     0.7404    0.7396    0.7393     56000\n",
            "\n",
            "Epoch 3/5 | Train Loss: 0.6721 | Train Acc: 0.5622 | Val Loss: 0.5799 | Val Acc: 0.7396 | Val F1: 0.7310 | Val Precision: 0.7549 | Val Recall: 0.7086\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7191    0.1449    0.2412     28035\n",
            "         1.0     0.5239    0.9433    0.6736     27965\n",
            "\n",
            "    accuracy                         0.5436     56000\n",
            "   macro avg     0.6215    0.5441    0.4574     56000\n",
            "weighted avg     0.6216    0.5436    0.4572     56000\n",
            "\n",
            "Epoch 4/5 | Train Loss: 0.6125 | Train Acc: 0.6644 | Val Loss: 0.6778 | Val Acc: 0.5436 | Val F1: 0.6736 | Val Precision: 0.5239 | Val Recall: 0.9433\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.6159    0.6773    0.6451     28035\n",
            "         1.0     0.6406    0.5766    0.6069     27965\n",
            "\n",
            "    accuracy                         0.6270     56000\n",
            "   macro avg     0.6282    0.6269    0.6260     56000\n",
            "weighted avg     0.6282    0.6270    0.6260     56000\n",
            "\n",
            "Epoch 5/5 | Train Loss: 0.6363 | Train Acc: 0.6175 | Val Loss: 0.6571 | Val Acc: 0.6270 | Val F1: 0.6069 | Val Precision: 0.6406 | Val Recall: 0.5766\n",
            "\n",
            " Training completed in 3526.00 seconds.\n",
            "--------------------------------------------------\n",
            "\n",
            " Retraining RNNModel using Train + Validation Data...\n",
            "\n",
            "\n",
            " Training RNNModel on Full Train (Train+Val) for 5 epochs...\n",
            "\n",
            "Epoch 1/5 | Train Loss: 0.5766 | Train Acc: 0.7217\n",
            "Epoch 2/5 | Train Loss: 0.5845 | Train Acc: 0.7140\n",
            "Epoch 3/5 | Train Loss: 0.5306 | Train Acc: 0.7660\n",
            "Epoch 4/5 | Train Loss: 0.6198 | Train Acc: 0.6228\n",
            "Epoch 5/5 | Train Loss: 0.6772 | Train Acc: 0.5359\n",
            "\n",
            " Training completed in 3877.34 seconds.\n",
            "\n",
            " Final Testing on Test Set...\n",
            "\n",
            " Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.5225    0.5666    0.5437     19000\n",
            "         1.0     0.5266    0.4822    0.5034     19000\n",
            "\n",
            "    accuracy                         0.5244     38000\n",
            "   macro avg     0.5246    0.5244    0.5235     38000\n",
            "weighted avg     0.5246    0.5244    0.5235     38000\n",
            "\n",
            "Final Test Results: Accuracy: 0.5244, Loss: 0.6845 | Precision: 0.5266 | Recall: 0.4822 | F1-score: 0.5034\n",
            "--------------------------------------------------\n",
            "\n",
            " Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.5225    0.5666    0.5437     19000\n",
            "         1.0     0.5266    0.4822    0.5034     19000\n",
            "\n",
            "    accuracy                         0.5244     38000\n",
            "   macro avg     0.5246    0.5244    0.5235     38000\n",
            "weighted avg     0.5246    0.5244    0.5235     38000\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            " Training RNN_W2V...\n",
            "\n",
            "\n",
            " Training RNNModel on Train Only for 5 epochs...\n",
            "\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.6403    0.7962    0.7098     28035\n",
            "         1.0     0.7297    0.5517    0.6283     27965\n",
            "\n",
            "    accuracy                         0.6741     56000\n",
            "   macro avg     0.6850    0.6739    0.6691     56000\n",
            "weighted avg     0.6850    0.6741    0.6691     56000\n",
            "\n",
            "Epoch 1/5 | Train Loss: 0.6770 | Train Acc: 0.5572 | Val Loss: 0.6177 | Val Acc: 0.6741 | Val F1: 0.6283 | Val Precision: 0.7297 | Val Recall: 0.5517\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7580    0.8173    0.7865     28035\n",
            "         1.0     0.8012    0.7384    0.7685     27965\n",
            "\n",
            "    accuracy                         0.7779     56000\n",
            "   macro avg     0.7796    0.7778    0.7775     56000\n",
            "weighted avg     0.7796    0.7779    0.7775     56000\n",
            "\n",
            "Epoch 2/5 | Train Loss: 0.6045 | Train Acc: 0.6555 | Val Loss: 0.5030 | Val Acc: 0.7779 | Val F1: 0.7685 | Val Precision: 0.8012 | Val Recall: 0.7384\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.6127    0.9467    0.7440     28035\n",
            "         1.0     0.8823    0.4001    0.5506     27965\n",
            "\n",
            "    accuracy                         0.6738     56000\n",
            "   macro avg     0.7475    0.6734    0.6473     56000\n",
            "weighted avg     0.7473    0.6738    0.6474     56000\n",
            "\n",
            "Epoch 3/5 | Train Loss: 0.5779 | Train Acc: 0.6915 | Val Loss: 0.5925 | Val Acc: 0.6738 | Val F1: 0.5506 | Val Precision: 0.8823 | Val Recall: 0.4001\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7172    0.8474    0.7769     28035\n",
            "         1.0     0.8130    0.6651    0.7316     27965\n",
            "\n",
            "    accuracy                         0.7563     56000\n",
            "   macro avg     0.7651    0.7562    0.7543     56000\n",
            "weighted avg     0.7650    0.7563    0.7543     56000\n",
            "\n",
            "Epoch 4/5 | Train Loss: 0.6047 | Train Acc: 0.6645 | Val Loss: 0.5441 | Val Acc: 0.7563 | Val F1: 0.7316 | Val Precision: 0.8130 | Val Recall: 0.6651\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.8305    0.4547    0.5877     28035\n",
            "         1.0     0.6239    0.9070    0.7393     27965\n",
            "\n",
            "    accuracy                         0.6806     56000\n",
            "   macro avg     0.7272    0.6808    0.6635     56000\n",
            "weighted avg     0.7273    0.6806    0.6634     56000\n",
            "\n",
            "Epoch 5/5 | Train Loss: 0.5901 | Train Acc: 0.6854 | Val Loss: 0.5926 | Val Acc: 0.6806 | Val F1: 0.7393 | Val Precision: 0.6239 | Val Recall: 0.9070\n",
            "\n",
            " Training completed in 236.34 seconds.\n",
            "--------------------------------------------------\n",
            "\n",
            " Retraining RNNModel using Train + Validation Data...\n",
            "\n",
            "\n",
            " Training RNNModel on Full Train (Train+Val) for 5 epochs...\n",
            "\n",
            "Epoch 1/5 | Train Loss: 0.5653 | Train Acc: 0.7082\n",
            "Epoch 2/5 | Train Loss: 0.5481 | Train Acc: 0.7374\n",
            "Epoch 3/5 | Train Loss: 0.5695 | Train Acc: 0.6970\n",
            "Epoch 4/5 | Train Loss: 0.6029 | Train Acc: 0.6320\n",
            "Epoch 5/5 | Train Loss: 0.5421 | Train Acc: 0.7209\n",
            "\n",
            " Training completed in 249.40 seconds.\n",
            "\n",
            " Final Testing on Test Set...\n",
            "\n",
            " Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7965    0.3994    0.5320     19000\n",
            "         1.0     0.5992    0.8979    0.7188     19000\n",
            "\n",
            "    accuracy                         0.6487     38000\n",
            "   macro avg     0.6979    0.6487    0.6254     38000\n",
            "weighted avg     0.6979    0.6487    0.6254     38000\n",
            "\n",
            "Final Test Results: Accuracy: 0.6487, Loss: 0.6258 | Precision: 0.5992 | Recall: 0.8979 | F1-score: 0.7188\n",
            "--------------------------------------------------\n",
            "\n",
            " Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7965    0.3994    0.5320     19000\n",
            "         1.0     0.5992    0.8979    0.7188     19000\n",
            "\n",
            "    accuracy                         0.6487     38000\n",
            "   macro avg     0.6979    0.6487    0.6254     38000\n",
            "weighted avg     0.6979    0.6487    0.6254     38000\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            " Training LSTM_LE...\n",
            "\n",
            "\n",
            " Training LSTMModel on Train Only for 5 epochs...\n",
            "\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9573    0.9217    0.9392     28035\n",
            "         1.0     0.9243    0.9588    0.9412     27965\n",
            "\n",
            "    accuracy                         0.9402     56000\n",
            "   macro avg     0.9408    0.9402    0.9402     56000\n",
            "weighted avg     0.9408    0.9402    0.9402     56000\n",
            "\n",
            "Epoch 1/5 | Train Loss: 0.2953 | Train Acc: 0.8474 | Val Loss: 0.1512 | Val Acc: 0.9402 | Val F1: 0.9412 | Val Precision: 0.9243 | Val Recall: 0.9588\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9496    0.9505    0.9501     28035\n",
            "         1.0     0.9504    0.9495    0.9499     27965\n",
            "\n",
            "    accuracy                         0.9500     56000\n",
            "   macro avg     0.9500    0.9500    0.9500     56000\n",
            "weighted avg     0.9500    0.9500    0.9500     56000\n",
            "\n",
            "Epoch 2/5 | Train Loss: 0.1166 | Train Acc: 0.9558 | Val Loss: 0.1302 | Val Acc: 0.9500 | Val F1: 0.9499 | Val Precision: 0.9504 | Val Recall: 0.9495\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9515    0.9496    0.9505     28035\n",
            "         1.0     0.9496    0.9514    0.9505     27965\n",
            "\n",
            "    accuracy                         0.9505     56000\n",
            "   macro avg     0.9505    0.9505    0.9505     56000\n",
            "weighted avg     0.9505    0.9505    0.9505     56000\n",
            "\n",
            "Epoch 3/5 | Train Loss: 0.0765 | Train Acc: 0.9723 | Val Loss: 0.1321 | Val Acc: 0.9505 | Val F1: 0.9505 | Val Precision: 0.9496 | Val Recall: 0.9514\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9367    0.9597    0.9481     28035\n",
            "         1.0     0.9586    0.9350    0.9467     27965\n",
            "\n",
            "    accuracy                         0.9474     56000\n",
            "   macro avg     0.9477    0.9474    0.9474     56000\n",
            "weighted avg     0.9476    0.9474    0.9474     56000\n",
            "\n",
            "Epoch 4/5 | Train Loss: 0.0484 | Train Acc: 0.9833 | Val Loss: 0.1690 | Val Acc: 0.9474 | Val F1: 0.9467 | Val Precision: 0.9586 | Val Recall: 0.9350\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9436    0.9475    0.9455     28035\n",
            "         1.0     0.9471    0.9432    0.9452     27965\n",
            "\n",
            "    accuracy                         0.9453     56000\n",
            "   macro avg     0.9453    0.9453    0.9453     56000\n",
            "weighted avg     0.9453    0.9453    0.9453     56000\n",
            "\n",
            "Epoch 5/5 | Train Loss: 0.0303 | Train Acc: 0.9900 | Val Loss: 0.1805 | Val Acc: 0.9453 | Val F1: 0.9452 | Val Precision: 0.9471 | Val Recall: 0.9432\n",
            "\n",
            " Training completed in 3525.71 seconds.\n",
            "--------------------------------------------------\n",
            "\n",
            " Retraining LSTMModel using Train + Validation Data...\n",
            "\n",
            "\n",
            " Training LSTMModel on Full Train (Train+Val) for 5 epochs...\n",
            "\n",
            "Epoch 1/5 | Train Loss: 0.0390 | Train Acc: 0.9878\n",
            "Epoch 2/5 | Train Loss: 0.0237 | Train Acc: 0.9923\n",
            "Epoch 3/5 | Train Loss: 0.0166 | Train Acc: 0.9947\n",
            "Epoch 4/5 | Train Loss: 0.0126 | Train Acc: 0.9960\n",
            "Epoch 5/5 | Train Loss: 0.0105 | Train Acc: 0.9966\n",
            "\n",
            " Training completed in 3951.99 seconds.\n",
            "\n",
            " Final Testing on Test Set...\n",
            "\n",
            " Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9513    0.9261    0.9385     19000\n",
            "         1.0     0.9280    0.9526    0.9401     19000\n",
            "\n",
            "    accuracy                         0.9393     38000\n",
            "   macro avg     0.9396    0.9393    0.9393     38000\n",
            "weighted avg     0.9396    0.9393    0.9393     38000\n",
            "\n",
            "Final Test Results: Accuracy: 0.9393, Loss: 0.2591 | Precision: 0.9280 | Recall: 0.9526 | F1-score: 0.9401\n",
            "--------------------------------------------------\n",
            "\n",
            " Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9513    0.9261    0.9385     19000\n",
            "         1.0     0.9280    0.9526    0.9401     19000\n",
            "\n",
            "    accuracy                         0.9393     38000\n",
            "   macro avg     0.9396    0.9393    0.9393     38000\n",
            "weighted avg     0.9396    0.9393    0.9393     38000\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            " Training LSTM_W2V...\n",
            "\n",
            "\n",
            " Training LSTMModel on Train Only for 5 epochs...\n",
            "\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9306    0.9556    0.9430     28035\n",
            "         1.0     0.9543    0.9286    0.9413     27965\n",
            "\n",
            "    accuracy                         0.9421     56000\n",
            "   macro avg     0.9425    0.9421    0.9421     56000\n",
            "weighted avg     0.9424    0.9421    0.9421     56000\n",
            "\n",
            "Epoch 1/5 | Train Loss: 0.2329 | Train Acc: 0.8984 | Val Loss: 0.1461 | Val Acc: 0.9421 | Val F1: 0.9413 | Val Precision: 0.9543 | Val Recall: 0.9286\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9513    0.9456    0.9485     28035\n",
            "         1.0     0.9458    0.9515    0.9486     27965\n",
            "\n",
            "    accuracy                         0.9486     56000\n",
            "   macro avg     0.9486    0.9486    0.9486     56000\n",
            "weighted avg     0.9486    0.9486    0.9486     56000\n",
            "\n",
            "Epoch 2/5 | Train Loss: 0.1383 | Train Acc: 0.9462 | Val Loss: 0.1317 | Val Acc: 0.9486 | Val F1: 0.9486 | Val Precision: 0.9458 | Val Recall: 0.9515\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9653    0.9289    0.9468     28035\n",
            "         1.0     0.9313    0.9666    0.9486     27965\n",
            "\n",
            "    accuracy                         0.9477     56000\n",
            "   macro avg     0.9483    0.9477    0.9477     56000\n",
            "weighted avg     0.9484    0.9477    0.9477     56000\n",
            "\n",
            "Epoch 3/5 | Train Loss: 0.1235 | Train Acc: 0.9520 | Val Loss: 0.1343 | Val Acc: 0.9477 | Val F1: 0.9486 | Val Precision: 0.9313 | Val Recall: 0.9666\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9564    0.9467    0.9516     28035\n",
            "         1.0     0.9471    0.9568    0.9519     27965\n",
            "\n",
            "    accuracy                         0.9517     56000\n",
            "   macro avg     0.9518    0.9518    0.9517     56000\n",
            "weighted avg     0.9518    0.9517    0.9517     56000\n",
            "\n",
            "Epoch 4/5 | Train Loss: 0.1150 | Train Acc: 0.9553 | Val Loss: 0.1250 | Val Acc: 0.9517 | Val F1: 0.9519 | Val Precision: 0.9471 | Val Recall: 0.9568\n",
            "\n",
            " Validation Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9485    0.9572    0.9528     28035\n",
            "         1.0     0.9567    0.9479    0.9523     27965\n",
            "\n",
            "    accuracy                         0.9526     56000\n",
            "   macro avg     0.9526    0.9525    0.9526     56000\n",
            "weighted avg     0.9526    0.9526    0.9526     56000\n",
            "\n",
            "Epoch 5/5 | Train Loss: 0.1088 | Train Acc: 0.9582 | Val Loss: 0.1259 | Val Acc: 0.9526 | Val F1: 0.9523 | Val Precision: 0.9567 | Val Recall: 0.9479\n",
            "\n",
            " Training completed in 253.30 seconds.\n",
            "--------------------------------------------------\n",
            "\n",
            " Retraining LSTMModel using Train + Validation Data...\n",
            "\n",
            "\n",
            " Training LSTMModel on Full Train (Train+Val) for 5 epochs...\n",
            "\n",
            "Epoch 1/5 | Train Loss: 0.1075 | Train Acc: 0.9584\n",
            "Epoch 2/5 | Train Loss: 0.1033 | Train Acc: 0.9603\n",
            "Epoch 3/5 | Train Loss: 0.1002 | Train Acc: 0.9616\n",
            "Epoch 4/5 | Train Loss: 0.0980 | Train Acc: 0.9622\n",
            "Epoch 5/5 | Train Loss: 0.0963 | Train Acc: 0.9631\n",
            "\n",
            " Training completed in 261.75 seconds.\n",
            "\n",
            " Final Testing on Test Set...\n",
            "\n",
            " Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9579    0.9458    0.9518     19000\n",
            "         1.0     0.9465    0.9584    0.9524     19000\n",
            "\n",
            "    accuracy                         0.9521     38000\n",
            "   macro avg     0.9522    0.9521    0.9521     38000\n",
            "weighted avg     0.9522    0.9521    0.9521     38000\n",
            "\n",
            "Final Test Results: Accuracy: 0.9521, Loss: 0.1229 | Precision: 0.9465 | Recall: 0.9584 | F1-score: 0.9524\n",
            "--------------------------------------------------\n",
            "\n",
            " Test Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.9579    0.9458    0.9518     19000\n",
            "         1.0     0.9465    0.9584    0.9524     19000\n",
            "\n",
            "    accuracy                         0.9521     38000\n",
            "   macro avg     0.9522    0.9521    0.9521     38000\n",
            "weighted avg     0.9522    0.9521    0.9521     38000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison and Analysis"
      ],
      "metadata": {
        "id": "EQ82cFrylBK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert results dictionary to DataFrame\n",
        "results_df = pd.DataFrame.from_dict(results, orient=\"index\")\n",
        "results_df = results_df.sort_values(by=\"accuracy\", ascending=False)  # Sort by accuracy\n",
        "\n",
        "# Print the results table\n",
        "print(\"\\n Final Model Performance Comparison:\\n\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "id": "LRwZY0L9lEfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "906a9f7d-7eab-4095-a82a-8a3c245726e2"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Final Model Performance Comparison:\n",
            "\n",
            "                loss  accuracy  precision    recall  f1-score\n",
            "LSTM_W2V    0.122912  0.952132   0.946515  0.958421  0.952431\n",
            "LSTM_LE     0.259095  0.939316   0.927963  0.952579  0.940110\n",
            "LogReg_LE   0.211040  0.931237   0.929676  0.933053  0.931361\n",
            "LogReg_W2V  0.281639  0.894553   0.898729  0.889316  0.893998\n",
            "RNN_W2V     0.625798  0.648684   0.599220  0.897947  0.718782\n",
            "RNN_LE      0.684483  0.524395   0.526646  0.482158  0.503421\n"
          ]
        }
      ]
    }
  ]
}